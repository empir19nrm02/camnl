# -*- coding: utf-8 -*-
"""
Non-Linearity calibration for devices with charge accumulating sensor.

@author: Christian Schrader
@email: christian.schrader@ptb.de
"""

# import os
import sys
import logging

import numpy as np
import numpy.polynomial.polynomial as po

# import numpy.polynomial as pol

# import scipy as sp
from scipy.interpolate import CubicSpline  # , interp1d
from scipy.optimize import least_squares
from scipy import integrate

import matplotlib.pyplot as plt  # FIXME needed for estimate_B0 -> make plots external

import time
# import warnings
# import scipy.stats
# from functools import partial
# from scipy import integrate
# from scipy.sparse import lil_matrix, coo_matrix

from .data import NLSelectedData, NLResultData, PDNL_Correction

logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    # format='%(name)s (%(lineno)d) %(levelname)s - %(message)s',
    format="%(levelname)s: %(message)s",
)
logger = logging.getLogger("camlin")
logging.getLogger("matplotlib.font_manager").disabled = True


class NLCalibration:
    """Calibration class for signal-processing non-linearity.

    Parameters
    ----------
    max_sensor_value : int
        Maximum value that can be delivered as raw counts. Depends on the
        bit width of the A/D converter. The default is 4095 (12 Bit, 2**12 -1)
    spnl_refpoint : float, optional
        Count value (offset compensated by y0) that is used as
        reference point for calculating the refence value tho which the
        count rates of the integration time series is normalised.
        This need to be reached in all evaluated series! May be adjusted to
        regions with small curvature of the NL curve.
        The default is 2000.
    nlftype : str, optional
        Type of the model function used to describe the spnl-nl.
        Currently only "polynom" is supported. The default is "polynom".
    """

    nl_fun = None
    nlftype = ""
    nl_fun_kwparams = {}
    max_sensor_value = 0
    spnl_refpoint = 0
    nr_observations = None  # FIXME make private

    # """Algorithms to calculate nonlinearity of cameras and spectrometers.

    # Two components:
    #         1. NL of signal processing  -> sigproc or sp
    #         2. NL of pixel diode        -> pixdiode or pd
    # """

    def __init__(
        self,
        max_sensor_value=4095,
        spnl_refpoint=2000,
        nlftype="polynom",
        nl_fun_kwparams={},
    ):

        self.max_sensor_value = max_sensor_value
        self.spnl_refpoint = spnl_refpoint

        # self.nl_fun ist creator-object that builds interpolation object
        # from two series of x and y values, e.g. interp_fun = self.nl_fun(x, y)
        if nlftype == "polynom":
            self.nl_fun = Polynom
        # elif nlftype == 'cspline':
        #     self.nl_fun = self.cspline
        else:
            raise Exception("NL function type '{nlftyp}' not supported!")
        self.nlftype = nlftype
        self.nl_fun_kwparams = nl_fun_kwparams
        return

    def select_nl_data(
        self,
        series_dat,
        y0=0.0,
        inttime_limits=(0.0, np.inf),
        value_limits=(0.0, np.inf),
        delta_inc_perc=0.0,
        n_min=10,
    ):
        """Select and reorder data for nonlinearity calculation.

        Select data from each integration time series in series_dat that is usable
        for non-linearity calculation. If no additional criterions are given, the
        only limit is the maximum count value from the sensor. Only data points
        where all values are smaller than this value are used.

        The result data is ordered by region/pixel in the first dimension and
        integration time series in the second dimension. This reodering has to be
        applied to y0 too. Therefore it is given als input parameter and put into
        the respective NLSelectedData.

        Parameters
        ----------
        series_dat : list of NLInputData
            List of NLInputData objects, generated by child class of
            ImportNLCalibData_Base.
        y0 : int or iterable, optional
            Values for count offset, scalar value or list/array of values for each
            region. The order has to match the internal evaluation order, which
            is ensured when calulated with linear_interpolation_nlinputdata().
            The default is 0.0.
        inttime_limits : tuple, optional
            Limits for integration time: (min, max). The default is (0., np.inf).
        value_limits : tuple, optional
            Limits for count values: (min, max). The default is (0., np.inf).
        delta_inc_perc : float, optional
            Minimum percentage of the increase of the average counts relative to the
            initial increase at the start of the series. Used to cut of data points
            where the region/pixel saturates. The default is 0.0.
        n_min : int, optional
            Minimum number of remaining/usable data points in integration time
            series. If the number is below this value, the series is considered not
            helpful to describe the non-linearity. The default is 10.

        Returns
        -------
        selected_dat : list of list of NLSelectedData
            Result data, ordered by region/pixel in the first dimension and
            integration time series in the second dimension.
        """

        nr_of_regions = len(series_dat[0].rois)  # samples.shape[0]
        selected_dat = [None] * nr_of_regions
        # if maxval is None:
        #     maxval = self.max_sensor_value - 1
        # # assert min_required_end_value < maxval
        # assert maxval < self.max_sensor_value

        # handle None-values
        inttime_limits = self._apply_limit_defaults(inttime_limits)
        value_limits = self._apply_limit_defaults(value_limits)

        if np.isscalar(y0):
            y0 = np.full(nr_of_regions, y0, dtype=np.float32)
        else:
            y0 = np.asarray(y0)
            assert y0.ndim == 1
            assert len(y0) == nr_of_regions

        for region_dat in series_dat:  # über alle Serien loopen

            # -----------------------
            assert (region_dat.samples is None) != (region_dat.avgdat is None), \
                "Only samples or averaged input allowed!"
            if region_dat.samples is not None:
                # first dimension of region_dat.samples are regions,
                # then inttime, then repetitive measurements

                if region_dat.samples.shape[0] != nr_of_regions:
                    # simple test to detect malformed data
                    # the roi definitions in region_dat.rois should also be
                    # identical (same objects)
                    raise Exception("Number of regions changed!")

                print("Calculate mean and overload ratio...")
                # averaging over all data should be faster than
                # partial averaging inside the loop

                # average over repetitive samples in last dimension
                m_r = region_dat.samples.mean(axis=-1)
                overload_r = np.sum(region_dat.samples == self.max_sensor_value, axis=-1)
            else:
                m_r = region_dat.avgdat.mean
                overload_r = region_dat.avgdat.ovrld
                # FiXME: next steps whith use of averaged data not implemented!
                # filtering by limits needs to be done on averaged values instead
                # of samples

            # variance currently unused
            inttimes = region_dat.inttimes

            # if self.mode == 'pixel':
            # elif self.mode == 'region':
            # else:
            #     raise Exception("Unknown averaging mode: %s" %(mode))
            # print(m_r, v_r, overload_r, inttimes)

            print("Selecting data...")
            for i, (roi_samples, y0_r, roi_mean, roi_overload) in enumerate(
                zip(region_dat.samples, y0, m_r, overload_r)
            ):
                # roi_samples is an inttime series with shape
                # (<nr_of_inttime_steps>,<samples>)
                # selection along first dimension

                # tic = time.perf_counter()

                selected_bool = np.logical_and(
                    np.logical_and(
                        # chose only inttime steps where all samples
                        # are >= the lower limit and <= the upper limit.
                        # allows to filter out values at the upper and lower
                        # bounds.
                        np.logical_and(
                            np.all(roi_samples >= value_limits[0], axis=-1),
                            np.all(roi_samples <= value_limits[1], axis=-1),
                        ),
                        # only inttime steps where no pixel in the region
                        # in all samples was in overload.
                        # Otherwise there would be clipping which leads to
                        # change of the mean value.
                        # TODO: correction by overload ratio
                        np.logical_not(roi_overload > 0),
                    ),
                    np.logical_and(
                        inttimes >= inttime_limits[0],
                        inttimes <= inttime_limits[1],
                    ),
                )

                # limit to increasing values -> discard saturated samples
                if delta_inc_perc > 0:
                    m_diff = np.diff(roi_mean)
                    selected_increasing = np.concatenate(
                        (
                            [True],
                            (m_diff > (m_diff[0:5].mean() / delta_inc_perc)),
                        )
                    )
                    selected_bool = np.logical_and(selected_bool, selected_increasing)

                selected = np.argwhere(selected_bool).T[0]

                if len(roi_samples) == 0:
                    logger.debug(
                        "No usable data in series '%s'", series_dat.series_name
                    )
                    continue  # can happen with spectrometer data

                # FIXME: Testen auf meanvalue? sollte doch gar nicht sein
                # if roi_mean[-1] < min_required_end_value:
                #     logger.debug("Pixel value too low in region %d." % (i))
                #     continue
                if len(selected) < n_min:
                    logger.debug(
                        "Number of usable values too low (n=%d) in series %d!",
                        len(selected),
                        i,
                    )
                    continue

                sel_dat = NLSelectedData()
                sel_dat.mean = roi_mean[selected]
                sel_dat.y0 = y0_r
                sel_dat.inttimes = inttimes[selected]
                sel_dat.samples = roi_samples[selected]
                # debug-info:
                sel_dat.series_name = region_dat.series_name
                sel_dat.roi_index = i
                sel_dat.roi = region_dat.rois[i]

                try:
                    selected_dat[i].append(sel_dat)
                except AttributeError:
                    selected_dat[i] = [sel_dat]
        return selected_dat

    def calc_sigproc_nl(
        self,
        nlseldata,
        ref_point=None,
        ref_type="linear",  # currently no other types implemented
        scale_to_refpoint=True,
        # FIXME: wo konfiguriert man dann die order oder controlpoints?
        # -> self.nl_fun_kwparams
        timecorr=0.0,
        mode="samples",
        outlier_propability=0,
    ):
        """
        Calculate linearity of signal processing chain from selected data.

        Parameters
        ----------
        nlseldata : NLSelectedData
            Selected data from self.select_nl_data().
        ref_point : float, optional
            Reference point to normalize each series to.
            When None, self.spnl_refpoint is used.
        ref_type : str, optional
            Type of interpolation at ref_point. Currently only 'linear' is
            implemented. The default is "linear".
        timecorr : float, optional
            Time offset that is added to the integration times. Can be used to
            compensate real timing shifts or smear effects. The default is 0.0.
        mode : TYPE, optional
            DESCRIPTION. The default is "samples".
        outlier_propability : float, optional
            Propability when a relative nl datapoint is qualified as an
            outlier. In a given bin at the count scale
            (1-outlier_propability/2) datapoints are inside the interval
            (a, b) around the calculated nl-function and outlier_propability
            datapoints are outside. If this parameter is given and > 0,
            then these limits (a,b) are calculated for different regions
            in the count scale. This can be used to limit the plotted
            datapoints to the outliers. The default is 0 (disabled).

        Raises
        ------
        Exception
            If nlseldata does not contain usable data, an exception is raised.

        Returns
        -------
        res : NLResultData
            Collected data about the non-linearity calculation:

                - the nl-function itself (interpolator object)

            for plotting:
                - used datapoints used for their calculation
                - outlier-limits
                - info dict to use with infopicker module
                - inttimes of datapoints used for color mapping

        """
        assert ref_type in ["linear"]  # 'cspline' -> unstable results
        assert mode in ["samples", "mean"]

        if ref_point is None:
            ref_point = self.spnl_refpoint

        inttimes_scaled = []  # schlechte Bezeichnung, ist ja nicht scaled
        y_scaled = []
        y_rate_rel_scaled = []
        ref_fun_scaled = []
        ref_val_scaled = []
        label = []
        info = {}

        seq_id = 0
        n_nlseldata = len(nlseldata)
        # nlseldata is list of region data
        # region_dat is list of selected data
        for i, region_dat in enumerate(nlseldata):
            # FIXME: better progress info
            if i % 1000 == 0:
                print(f"calc_sigproc_nl() i: {i} / {n_nlseldata}")

            # ab hier muss y0 skalar sein, da pro Region

            if region_dat is None:
                print(f"Region {i} empty")
                continue

            for d in region_dat:

                # add (virtual) time correction to eliminate real timing
                # offsets or smear effect
                inttimes = d.inttimes + timecorr

                # calculate reference value by interpolating mean-series
                # at ref_point
                y = d.mean - d.y0  # over offset corrected counts
                y_rate = y / inttimes  # (y-y_ad)/t_i

                if ref_type == "linear":
                    # if multiple intersection at ref_point, then average
                    ref_value = self.interpolate_series(y, y_rate, ref_point).mean()
                # elif ref_type == 'cspline':
                #     # cspline is prone to interpolation errors on strong
                #     # local gradients. Don't use!
                #     interp = CubicSpline(X_lim, y_lim)
                #     ref_value = interp(ref_point)
                else:
                    raise Exception(f"unknown ref_type: {ref_type}")

                # normalize data to ref_value
                if mode == "mean":
                    y = d.mean - d.y0
                    y_rate = (d.mean - d.y0) / inttimes  # wie oben
                    y_rate_rel = y_rate / ref_value
                    inttimes2 = inttimes
                elif mode == "samples":
                    print("SAMPLE-MODE FUNKTIONIERT NICHT RICHTIG!")
                    y = (d.samples - d.y0).ravel()
                    # y_rate_rel = (
                    #     (d.samples - d.y0).T / inttimes / ref_value
                    # ).T.ravel()
                    # sollte identisch sein mit:
                    y_rate_rel = (
                        (d.samples - d.y0) / inttimes[:, np.newaxis] / ref_value
                    ).ravel()
                    # inttimes2 only used for plotting (color)
                    inttimes2 = (
                        np.ones_like(d.samples) * inttimes[:, np.newaxis]
                    ).ravel()

                # results can be reduced in representation width
                inttimes_scaled.append(inttimes2.astype(np.float32))
                # append maintains structure
                y_scaled.append(y)  # .astype(np.float32))
                y_rate_rel_scaled.append(y_rate_rel)  # .astype(np.float32))
                ref_val_scaled.append(ref_value)

                # string starting with _ to get ignored in legends
                # and using prefix to be used as selector
                label.append(f"_seq{seq_id}")

                # format info string
                if d.roi.begin_x == d.roi.end_x:
                    # txt_x = "x:{:d}".format(d.roi.begin_x)
                    txt_x = f"x:{d.roi.begin_x}"
                else:
                    # txt_x = "x:{:d}..{:d}".format(d.roi.begin_x, d.roi.end_x)
                    txt_x = f"x:{d.roi.begin_x}..{d.roi.end_x}"

                if d.roi.begin_y == d.roi.end_y:
                    # txt_y = "y:{:d}".format(d.roi.begin_y)
                    txt_y = f"y:{d.roi.begin_y}"
                else:
                    # txt_y = "y:{:d}..{:d}".format(d.roi.begin_y, d.roi.end_y)
                    txt_y = f"y:{d.roi.begin_y}..{d.roi.end_y}"
                # info_txt = (
                #     'Series: "{:s}" ROI: {:s} {:s} ROI-Index: {:d}'.format(
                #         d.series_name, txt_x, txt_y, d.roi_index
                #     )
                # )
                info_txt = (
                    f'Series: "{d.series_name}" '
                    f"ROI: {txt_x} {txt_y} ROI-Index: {d.roi_index}"
                )

                info[seq_id] = info_txt
                seq_id += 1

        # nl_coeff = None
        nlfunc = None
        if len(y_scaled) != 0 and len(y_rate_rel_scaled) != 0:
            x_a = np.concatenate(y_scaled)
            y_a = np.concatenate(y_rate_rel_scaled)
            nlfunc = self.nl_fun(x_a, y_a, **self.nl_fun_kwparams)
            if scale_to_refpoint:
                refpoint_val = nlfunc(ref_point)
                y_a = y_a / refpoint_val

                # dirty hack! not portable, accesses internal parameter, works only with
                # Polynom class
                # nlfunc._params = nlfunc._params / refpoint_val

                # variant 2, refit on scaled y_a
                # portable, does not access internal parameter
                nlfunc = self.nl_fun(x_a, y_a, **self.nl_fun_kwparams)

            # nl_coeff = po.polyfit(x_a, y_a, order)
            # nl_coeff = list(pol.Polynomial.fit(x_a, y_a, order))
            # csp = self.fit_cspline(x_a, y_a)

            if outlier_propability > 0:
                assert outlier_propability < 1
                lim_x, lim_up, lim_lo = self.get_outlier_limits(x_a, y_a, nlfunc)
            else:
                lim_x, lim_up, lim_lo = [], [], []
        else:
            raise Exception("No data for NL calculation!")

        """
        FIXME: man könnte hier gleich alle Daten und nl_fun auf den Wert der nl_fun
        beim refpoint normieren. Dann erhält man Daten und nl_fun so, dass
        nl_fun(refpoint) == 1 ist

        oder verschiebt man nur? Unterschied sollte minimal sein.
        Skalieren ändert die Amplitude der nl_fun, verschieben nicht

        """

        #  Speicherformat für traditionelle NL-Berechnung
        res = NLResultData()
        res.inttimes = inttimes_scaled
        res.ref_val = ref_val_scaled
        res.ref_point = ref_point
        res.x = y_scaled
        res.y = y_rate_rel_scaled
        res.ref_fun = ref_fun_scaled
        # use objects not polynome coefficients
        # res.nl_coeff = nl_coeff
        res.nl_fun = nlfunc
        res.outlier_limits = (lim_x, lim_lo, lim_up)
        res.use_raw_counts = False
        res.label = label
        res.info = info
        return res

    def calc_y0(self, dat, max_linear_value=0):
        """
        Calculate offset counts for integration time -> 0.

        Parameters
        ----------
        dat : NLInputData
            List of NLInputData objects, generated by child class of
            ImportNLCalibData_Base. Usually dark data.
        max_linear_value : TYPE, optional
            Maximum value up to where the curve can be regarded as linear,
            respective to the origin. The default is 0 (unlimited).

        Returns
        -------
        TYPE
            DESCRIPTION.

        """
        # logger.warning("calc_y0() deprecated!")
        return self.linear_regression(dat, max_linear_value=max_linear_value)[0]

    @staticmethod
    def linear_regression(nlinputdata, max_linear_value=0):
        """Linear interopolation of sample points in input data.

        For each reagion the data of integration time series gets linear
        interpolated and the linear coefficients for each region/pixel are
        returned.


        Parameters
        ----------
        nlinputdata : NLInputData
            List of NLInputData objects, generated by child class of
            ImportNLCalibData_Base.
        max_linear_value : int, optional
            Maximum value up to where the curve can be regarded as linear,
            respective to the origin. The default is 0 (unlimited).

        Returns
        -------
        linear_coefficients : (2,n) ndarray
            The first row corresponds to the count offset y0 and the
            second to the thermal or photonic count rate (A, B), depending on the
            type of input data (dark, bright). The column corresponds to the
            region/pixel.

            The evaluation order ist the same as in other parts of the module.
        """

        # try:
        #     nlinputdata[0]  # is iterable?
        # except TypeError:
        #     # make it iterable.
        #     nlinputdata = [nlinputdata]

        # Test for instances of NLInputData?

        # sanity check for same number of regions
        all_equal = True
        g = (len(d.samples) for d in nlinputdata)
        first = next(g)
        for x in g:
            if x != first:
                all_equal = False
                break
        assert all_equal

        linreg_dat = [None] * first

        for d in nlinputdata:
            if max_linear_value > 0:
                selected_all = np.all(d.samples <= max_linear_value, axis=2)

            # calculation using averaged values gives same result than using
            # raw values but is a little faster.
            x = []
            y = []
            for i, samples in enumerate(d.samples.mean(axis=2)):
                # loop over regions
                if max_linear_value > 0:
                    selected = selected_all[i]
                    x = d.inttimes[selected]
                    y = samples[selected]
                else:
                    x = d.inttimes
                    y = samples
                if linreg_dat[i] is None:
                    linreg_dat[i] = ([], [])  # [x, y]
                linreg_dat[i][0].extend(x)
                linreg_dat[i][1].extend(y)

        lincoeff = []
        for x, y in linreg_dat:
            lincoeff.append(po.polyfit(x, y, 1))
        lincoeff = np.asarray(lincoeff, dtype=np.float32).T
        return lincoeff  # [0] -> y0,  [1] -> count rate

    def interpolate_series(self, x, y, xref):
        """Robust linear interpolation with non-monotone data.

        Interpolation of count rate series (y / ti) at a given reference point xref
        can be problematic if x is not monotone. Functions like
        scipy.interpolate.interp1d() sort the data. Here this is not helpful.
        There can be multiple crossings over xref inside of x between consecutive
        elements. Each of these segments gets lin. interpolated individually and
        the list of values is returned. Usually the averaged value is used as
        overall interpolation value.

        Parameters
        ----------
        x : (n,) ndarray
            x data
        y : (n,) ndarray
            y data
        xref : TYPE
            reference point in x where y should be interpolated.

        Returns
        -------
        (n,) ndarray
            array of interpolation points.
        """

        int_dat = np.asarray(
            [
                [a_x, b_x, a_y, b_y]
                for a_x, b_x, a_y, b_y in zip(x[:-1], x[1:], y[:-1], y[1:])
                if (a_x <= xref and b_x > xref) or (a_x > xref and b_x <= xref)
            ]
        )
        if not int_dat.size:  # len(int_dat) == 0
            # no embracing interpolation tuple found
            # xref not in x.min()..x.max()
            return None
        int_vals = self._interpolate(int_dat, xref)
        return int_vals  # FIXME: return int_dat too?

    @staticmethod
    def _interpolate(dat, xref):
        """Vectorized interpolation.

        Parameters
        ----------
        dat : (n,4) ndarray
              every row [a_x, b_x, a_y, b_y]
        xref : float
               interpolation position, may be (n,) ndarray

        Returns
        -------
        y_interpolated : (n,) ndarray
        """
        a_x = dat[:, 0]
        b_x = dat[:, 1]
        a_y = dat[:, 2]
        b_y = dat[:, 3]
        return a_y + (xref - a_x) * (b_y - a_y) / (b_x - a_x)

    @staticmethod
    def get_outlier_limits(x_a, y_a, nl_fun, n_bins=50, alpha=0.01):
        """Determine upper and lower limit of nl data.

        Values outside limits may be outlier / abberant data.
        Plots can be limited to outlier for performance reasons.
        """
        quantile = 1 - alpha / 2
        sortidx = np.argsort(x_a)
        x_a = x_a[sortidx]
        y_a = y_a[sortidx]
        del sortidx

        bin_edges = np.linspace(x_a[0], x_a[-1], num=n_bins + 1)
        # Intervallmitten
        x_mid = (x_a[-1] - x_a[0]) / n_bins / 2 + bin_edges[:-1]
        y_mid = nl_fun(x_mid)
        y_center = nl_fun(x_a)

        j = 1
        limit = bin_edges[j]
        edges = [0]

        y = []
        quant_a = []
        for i, x in enumerate(x_a):
            if x < limit:
                y.append(y_a[i])
                continue

            # in Bin in der Mitte mal "merken", um Histogramm zu erstellen
            # if i == 100:
            #    y_foo = np.array(y)
            y_diff = y - y_center[i]
            quant_upper = np.quantile(y_diff, quantile)
            quant_lower = np.quantile(-y_diff, quantile)
            quant_a.append((quant_upper, quant_lower))
            y = []
            j += 1
            edges.append(i)
            if j == n_bins:
                edges.append(j + 1)
                break
            limit = bin_edges[j]

        quant_a = np.asarray(quant_a)
        y_up = y_mid[:-1] + quant_a[:, 0]
        y_lo = y_mid[:-1] - quant_a[:, 1]
        x_spl = np.linspace(x_a[0], x_a[-1], num=200)
        y_spl_up = CubicSpline(x_mid[:-1], y_up, bc_type="natural")(x_spl)
        y_spl_lo = CubicSpline(x_mid[:-1], y_lo, bc_type="natural")(x_spl)
        return x_spl, y_spl_lo, y_spl_up

    # FIXME: Name unzureichend. Fittet nicht den CSpline selber, sondern passt
    # dessen Controlpunkte an Daten an. Schwer zu beschreiben.
    def res_fit_cspline(self, param, *args):
        """ "Residue function for cspline fit."""
        x, y, control_x = args
        fun = CubicSpline(control_x, param, bc_type="natural", extrapolate=True)
        res = y - fun(x)
        return res

    def fit_cspline(self, x, y):
        """Fits c-spline in data, controled by set of control points.

        Parameters
        ----------
            x, y : data point to be fitted
                control_x : x positions of control points

        Returns
        -------
        cspline : CubicSpline-interpolator object

        """

        assert len(x) == len(y)

        control_x = np.linspace(x.min(), x.max(), num=10)

        param = np.ones_like(control_x)

        # print("Startparameter:")
        # print("===============")
        np.set_printoptions(precision=3)
        print("#nlparam:", len(param))
        print("Observations:", len(x))

        args = [x, y, control_x]

        res = self.res_fit_cspline(param, *args)
        res_sum = (res**2).sum()
        print(
            "Residue²-Sum:",
            res_sum,
            "mean squared error:",
            res_sum / (len(res) - len(param)),
        )

        fit_res = least_squares(
            fun=self.res_fit_cspline,
            method="trf",  # trf lm dogbox
            x0=param,
            args=args,  # Mess- und Hilfsdaten
            x_scale="jac",
            verbose=2,
        )
        # print("Endparameter:")
        # print("===============")
        # print(fit_res.x)
        res_sum = (fit_res.fun**2).sum()
        print(
            "Residue²-Sum:",
            res_sum,
            "mean squared error:",
            res_sum / (len(res) - len(fit_res.x)),
        )
        print()
        cspline = CubicSpline(control_x, fit_res.x, bc_type="natural", extrapolate=True)
        return cspline

    @staticmethod
    def _apply_limit_defaults(lim):
        """Replace None in limits to numerical defaults."""
        lim = list(lim)  # make it mutable
        if lim[0] is None:
            lim[0] = 0
        if lim[1] is None:
            lim[1] = np.inf
        return tuple(lim)

    def fun_quadratic(self, param, y):
        return param[0] + param[1] * (y**2)

    def fun_exponential(self, param, y):
        B0, a1, a2 = param
        return B0 - a1 * np.exp(a1 * y)

    def estimate_exponential_params(self, Y, B, B0):
        # last point has expected maximal drop
        y1 = Y[-1]
        b1 = B[-1]
        c_a = []
        a_a = []
        for y2, b2 in zip(
            Y[len(Y) // 2 : -1], B[len(B) // 2 : -1]
        ):  # von mitte bis vorletztes Element
            c = (np.log(B0 - b2) - np.log(B0 - b1)) / (y2 - y1)
            a = (B0 - b1) / np.exp(c * y1)
            c_a.append(c)
            a_a.append(a)
        return np.asarray(a_a), np.asarray(c_a)

    # def res_estimate_B0(self, param, y, B):
    #     # return self.fun_quadratic(param, y) - B
    #     return self.fun_exponential(param, y) - B

    # def _fit_B0(self, y, B):
    #     fitRes = least_squares(
    #         # fun=lambda  p, x, b: self.fun_exponential(p, x) - b,
    #         fun=lambda p, x, b: self.fun_quadratic(p, x) - b,
    #         method="trf",
    #         # x0=[B.mean(), .1, 1e-4],
    #         x0=[B.mean(), 0],
    #         args=(y, B),
    #         x_scale="jac",
    #         verbose=0,
    #     )
    #     # rebuild quadratic function with final parameters for plotting
    #     self.dbg_quadratic = self.fun_quadratic(fitRes.x, y)
    #     self.fitRes_B = fitRes
    #     self.dbg_y = y
    #     self.dbg_B = B
    #     return fitRes.x[0]  # x0 is B(y=0)

    def estimate_B0(self, dat_lincorr, mode="mean", update_dat=True):
        assert np.all([x is not None for x in dat_lincorr])
        assert mode in ["samples", "mean"]

        model_fun = self.fun_quadratic
        #  model_fun = self.fun_exponential
        res_fun = lambda p, x, b: model_fun(p, x) - b
        x0 = [0, 0]
        # x0 = [0, .1, 1e-4]

        # FIXME: Plotten als Option
        fig = plt.figure("Estimation B")
        ax = fig.gca()
        B = []
        for region_dat in dat_lincorr:
            for series_dat in region_dat:
                # selected = np.argwhere(
                #     series_dat.mean <= self.max_linear_value
                # ).T[0]
                ti = (
                    series_dat.inttimes
                )  # FIXME: evtl. timeoffset berücksichtigen, auch wenn es nichts ausmachen sollte

                if mode == "mean":
                    Y = series_dat.mean - series_dat.y0
                elif mode == "samples":
                    Y = series_dat.samples - series_dat.y0
                    ti = np.tile(ti, (Y.shape[1], 1)).T  # expand to dimension of Y
                    # now make 1D
                    ti = ti.ravel()
                    Y = Y.ravel()

                B_ser = Y / ti

                x0[0] = B_ser.mean()  # max()?
                fitRes = least_squares(
                    fun=res_fun,
                    method="lm",
                    x0=x0,
                    args=(Y, B_ser),
                    x_scale="jac",
                    verbose=0,
                )
                # rebuild quadratic function with final parameters for plotting
                # self.dbg_quadratic = self.fun_quadratic(fitRes.x, y)
                # self.fitRes_B = fitRes
                # self.dbg_y = y
                # self.dbg_B = B

                B.append(fitRes.x[0])
                if update_dat:
                    series_dat.B0 = fitRes.x[0]  # insert into dat_bright
                line = ax.plot(Y, B_ser, ".")
                ax.plot(
                    Y,
                    model_fun(fitRes.x, Y),
                    "--",
                    c=line[0].get_color(),
                )
        # if update_dat:
        #     return series_dat
        return np.asarray(B)

    def estimate_fit_params(self, B, saturation_factor=1):
        # yratio_max = .7V/.026V = 27
        C = self.max_sensor_value / 27 * saturation_factor
        J = np.mean(B) / np.exp(27)
        return J, C

    def pixelcountrate(self, t, y, B, J, C):
        # J and C are nlfuncparam
        return B - J * (np.exp(y[0] / C) - 1)

    def res_diode_nl(self, x, *args):
        (regdata,) = args

        if self.nr_observations is None:  # first run
            self.nr_observations = 0
            for rdat in regdata.dat_bright:
                for d in rdat:
                    self.nr_observations += len(d.mean)
        if self.nr_observations == 0:
            raise Exception("No observations!")
        res = np.zeros(self.nr_observations)

        i = 0  # pointer at position in res vector
        i_B = 0  # pointer at sequence
        for i_reg, rdat in enumerate(regdata.dat_bright):
            # rdat is region-data -> contains multiple sequences
            for d in rdat:
                # d is sequence data of one inttime series for one region

                ti = d.inttimes
                if getattr(regdata, "timeoffset") is not None:
                    ti += regdata.timeoffset

                # sigproc-nl already compensated

                # per mean
                y = d.mean - d.y0
                y_lin = y

                # samples, sollte sauberer sein, da jeder Messwert mit
                # zugehöriger NL korrigiert wird
                # y = d.samples-d.y0
                # k_rel = regdata.nl_fun(y)
                # y_lin = (y / k_rel).mean(axis=-1)

                # Modell
                # integrieren
                solvres = integrate.solve_ivp(
                    fun=self.pixelcountrate,
                    # integrate from ti=0 to ti_max
                    t_span=(0, ti[-1]),
                    y0=[0],
                    args=(
                        d.B0,
                        x[0],  # J
                        x[1],  # C
                    ),
                    max_step=ti[-1] / 100,  # limit stepsize
                    method="RK45",
                    # evaluate at measured integration times
                    t_eval=ti,
                )
                # print(solvres.success)
                # print("L", len(k_rel), len(solvres.y[0]))
                model = solvres.y[0]

                # print("L", len(model), i, len(res))
                n_obs = len(model)
                i_end = i + n_obs
                # print("M", model-y)
                res[i:i_end] = model - y_lin
                i = i_end
                # Sequenzzähler, nur hier bei Hellsignal hochzählen
                i_B = i_B + 1

        # # Constraints
        # contr = self.constraint_fun(param.nlfuncparam)
        # res = np.concatenate((res, contr))
        self.res = res
        return res

    # def constraint_fun(self, fun_param):
    #     # krel[refpoint] needs to be 1
    #     constr = (regdata.nl_fun(self.spnl_refpoint) - 1) * 1e6
    #     return np.atleast_1d(constr)

    def do_fit_nl(self, nldiodeparam, regdata):
        # global fitRes, bounds
        res_fun = self.res_diode_nl

        # important! otherwise it uses nr of last call!
        self.nr_observations = None

        # args = (nldiodeparam, regdata)
        args = (regdata,)

        print("Start Parameters:")
        print("===============")
        np.set_printoptions(precision=3)
        # print("#nld:", len(param.nldiodeparam))
        print("#nld:", len(nldiodeparam))

        # res = res_fun(param.param_vect, *args)
        res = res_fun(nldiodeparam, *args)
        print(
            "Observations:",
            self.nr_observations,
        )
        res_sum = (res**2).sum()
        print(
            "Residue²-Sum:",
            res_sum,
            "Mean Quadr. Error:",
            # res_sum / (len(res) - len(param.param_vect)),
            res_sum / (len(res) - len(nldiodeparam)),
        )

        # beim IVP-Integrator bei Exceptions anhalten, context-manager nur
        # einmal setzen, nicht in res_nonlinearity
        # with np.errstate(
        #     divide="raise", over="raise", under="raise", invalid="raise"
        # ):
        #     pass
        fitRes = least_squares(
            fun=res_fun,
            method="trf",
            # x0=param.param_vect,
            x0=nldiodeparam,
            args=args,
            # jac='3-point',
            x_scale="jac",
            # max_nfev=100,
            verbose=2,
        )
        print("End Parameters:")
        print("===============")
        res_sum = (fitRes.fun**2).sum()
        print(
            "Residue²-Sum:",
            res_sum,
            "Mean Quadr. Error:",
            res_sum / (len(res) - len(fitRes.x)),
        )
        return fitRes

    # def cspline(self, x, c, reuse=False):
    #     if not reuse:
    #         # an festen Stellen interpolieren
    #         xx = self._cspline_x(len(c))
    #         # die im Fit angepassten Werte sind die Parameter in c
    #         self._cspline = CubicSpline(xx, c)
    #     return self._cspline(x)

    # def _cspline_x(self, n):
    #     return np.linspace(
    #         0, self.max_sensor_value, num=n, endpoint=True, dtype=int
    #     )

    def set_sigproc_nl(self, fun):
        self.sigproc_nl = fun

    def correct_sigproc_nl(self, y, y0=0.0, nl_fun=None):
        if nl_fun is None:
            nl_fun = self.sigproc_nl
        assert nl_fun is not None
        if isinstance(y, np.ndarray):
            y = y - y0
            return y / nl_fun(y) + y0
        elif isinstance(y, list):
            y = np.asarray(y) - y0
            return list(y / nl_fun(y) + y0)
        else:
            raise Exception("Unsupported data type for x!")
        return

    def correct_sigproc_nl_selected_data(self, dat, nl_fun=None, in_place=True):
        # correct in-place, to save memory
        if nl_fun is None:
            nl_fun = self.sigproc_nl

        dat_new = []
        for region_dat in dat:
            rd_new = []
            for series_dat in region_dat:
                if not isinstance(series_dat, NLSelectedData):
                    raise Exception("Input not of type NLSelectedData!")
                if in_place:
                    series_dat.samples = self.correct_sigproc_nl(
                        series_dat.samples, nl_fun
                    )
                    # recalculate, not just scale the means
                    series_dat.mean = series_dat.samples.mean(axis=-1)
                else:
                    # partial copy
                    new = NLSelectedData()
                    new.B0 = series_dat.B0
                    new.inttimes = series_dat.inttimes
                    new.roi = series_dat.roi
                    new.roi_index = series_dat.roi_index
                    new.series_name = series_dat.series_name
                    new.var = series_dat.var  # not used
                    new.y0 = series_dat.y0
                    new.samples = self.correct_sigproc_nl(
                        series_dat.samples.copy(), y0=new.y0, nl_fun=nl_fun
                    )
                    new.mean = new.samples.mean(axis=-1)
                    rd_new.append(new)
            dat_new.append(rd_new)
        if in_place:
            return dat
        return dat_new


class Polynom:
    """Creator of interpolator object.

    Parameters
    ----------
    x: (n,) ndarray
    y: (n,) ndarray
    order: int, Default: 5
    """

    def __init__(self, x, y, order=5, x0=0.):
        # FIXME: wie order konfigurierbar machen?
        self._x0 = x0
        _x = np.asarray(x) - self._x0
        self._params = po.polyfit(_x, y, order)

    def __call__(self, x):
        """Make object callable with fixed parameter set."""
        _x = np.asarray(x) - self._x0
        return po.polyval(_x, self._params)


# class CSpline(object):
#     def __init__(self, x, y):
#         assert len(x) == len(x)
#         self._cspline = CubicSpline(x, y)

#     def __call__(self, x):
#         return self._cspline(x)
